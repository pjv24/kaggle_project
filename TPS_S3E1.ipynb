{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Task**\n",
        "####It is a regression problem to predict the median house price. \n",
        "**Dataset**\n",
        "####Both train and test dataset is generated from a deep learning model trained on California Housing Dataset\n",
        "**Attributes**\n",
        "\n",
        "####MedInc - Median income for households within a block of houses.\n",
        "####HouseAge - Age of a house within a block; a lower number is a newer building.\n",
        "####AveRooms - Average number of rooms within a block.\n",
        "####AveBedrms - Average number of bedrooms within a block.\n",
        "####Population - Total number of people residing within a block.\n",
        "####AveOccup - Average number of household members.\n",
        "####Longitude - A measure of how far west a house is; a more negative value is farther west.\n",
        "####Latitude - A measure of how far north a house is; a higher value is farther north.\n",
        "####MedHouseValue - Median house value for households within a block."
      ],
      "metadata": {
        "id": "1-1xwA8ExexP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Library imports**"
      ],
      "metadata": {
        "id": "fTslB6rfbZX8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np             #provides support for multi-dimensional arrays and mathematical functions.\n",
        "import pandas as pd            #provides data manipulation and analysis tools.\n",
        "import matplotlib.pyplot as plt #plotting library for creating visualizations\n",
        "import seaborn as sns           #data visualization library based on matplotlib, providing additional tools for creating statistical graphics\n",
        "%matplotlib inline              "
      ],
      "metadata": {
        "id": "rFXPAJb40NZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exploratory Data Analysis**"
      ],
      "metadata": {
        "id": "tAPj8VoYbtUW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#reading input file\n",
        "train_df=pd.read_csv(\"train_data.csv\")\n",
        "test_df=pd.read_csv(\"test_data.csv\")"
      ],
      "metadata": {
        "id": "lLM_JIs4buoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Analysing the data"
      ],
      "metadata": {
        "id": "JGG9rYYj58-H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.head()   # displays the first five rows of the dataframe"
      ],
      "metadata": {
        "id": "FQOZKLPl5oET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.head()   # displays the first five rows of the dataframe"
      ],
      "metadata": {
        "id": "FF-xQdGS6NS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.shape    # returns a tuple of array dimension that specifies the number of rows and columns"
      ],
      "metadata": {
        "id": "xl25R94O6S1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.shape    # returns a tuple of array dimension that specifies the number of rows and columns"
      ],
      "metadata": {
        "id": "yVIHsN8y6XBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.info()   # prints the information about the dataframe"
      ],
      "metadata": {
        "id": "tZoWdF-G6e7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.info()   # prints the information about the dataframe"
      ],
      "metadata": {
        "id": "eQjli6hB6mX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check for missing values and handle them"
      ],
      "metadata": {
        "id": "Sqv6uoG06p4P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.isnull().sum()"
      ],
      "metadata": {
        "id": "kvDIrQb36nSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.isna().sum()"
      ],
      "metadata": {
        "id": "llJTe_Od62IT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check for duplicate values"
      ],
      "metadata": {
        "id": "-QS2ls14670B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.duplicated().any()"
      ],
      "metadata": {
        "id": "I0uK_BN-65td"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.duplicated().any()"
      ],
      "metadata": {
        "id": "ZYPBEEVI7B-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check for outliers"
      ],
      "metadata": {
        "id": "KgmiLkde7Jkf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.skew()"
      ],
      "metadata": {
        "id": "LfFGjNAI7E60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import skew\n",
        "for column in df_train:\n",
        "    print(column)\n",
        "    print(f\"Skewness: {skew(df_train[column])}\")\n",
        "    plt.figure(figsize=(3,3))\n",
        "    plt.style.use('ggplot')\n",
        "    sns.distplot(df_train[column])\n",
        "    plt.grid(False)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "QJWgwOxS7OOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above code uses the **scipy.stats.skew** function to calculate the skewness of each column in a DataFrame df_train. Skewness measures the degree of asymmetry of a distribution, with positive values indicating a right-skewed (long tail to the right) distribution and negative values indicating a left-skewed (long tail to the left) distribution.\n",
        "\n",
        "The code then creates a histogram of each column's distribution using **seaborn.distplot** and displays it using **matplotlib.pyplot.show**. **plt.grid**(False) is used to turn off the grid in the histogram, and **plt.style.use**('ggplot') applies a style to the plot.\n",
        "\n",
        "This code is useful for identifying columns with highly skewed distributions, which may indicate that they need to be transformed before modeling."
      ],
      "metadata": {
        "id": "ET6O0jHs7oK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#handle skewness\n",
        "#inter quartile range\n",
        "quantile1=df_train[\"AveBedrms\"].quantile(0.25)\n",
        "quantile2=df_train[\"AveBedrms\"].quantile(0.75)"
      ],
      "metadata": {
        "id": "zGUlrSnR8Qgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train[\"AveBedrms\"]=np.where(df_train[\"AveBedrms\"]<quantile1,quantile1,df_train[\"AveBedrms\"])\n",
        "df_train[\"AveBedrms\"]=np.where(df_train[\"AveBedrms\"]>quantile2,quantile2,df_train[\"AveBedrms\"])"
      ],
      "metadata": {
        "id": "xdtq8XM98kSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = round(df_train['AveBedrms'].skew(),6)\n",
        "print(a)"
      ],
      "metadata": {
        "id": "WXUfs09C8nyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above code calculates the first quartile (25th percentile) and third quartile (75th percentile) of the \"AveBedrms\" column in the DataFrame df_train and assigns them to variables quantile1 and quantile2, respectively.\n",
        "This code uses numpy.where to replace any values in the \"AveBedrms\" column of df_train that are below the first quartile (quantile1) with quantile1, and any values that are above the third quartile (quantile2) with quantile2.\n",
        "The code also calculates the skewness of the \"AveBedrms\" column in the DataFrame df_train using the skew() function from the Pandas library and assigns the result to variable a. The round() function is then used to round the result to 6 decimal places.\n",
        "\n",
        "Quartiles divide a dataset into four equal parts, with the first quartile being the value below which 25% of the data falls and the third quartile being the value below which 75% of the data falls. The range between the first and third quartiles is known as the interquartile range (IQR) and is often used to identify outliers in a dataset. By setting extreme values to the quartile values, the data is brought back within a more reasonable range, which can improve the accuracy of models and statistical analysis. In this specific case, it is assumed that the outliers are values that are either too high or too low in the distribution, hence they are being replaced with the upper and lower bounds of the interquartile range.\n",
        "\n"
      ],
      "metadata": {
        "id": "oDJ61v_M9NHy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Interquantile Range\n",
        "quantile1=df_train[\"MedInc\"].quantile(0.25)\n",
        "quantile2=df_train[\"MedInc\"].quantile(0.75)\n",
        "df_train[\"MedInc\"]=np.where(df_train[\"MedInc\"]<quantile1,quantile1,df_train[\"MedInc\"])\n",
        "df_train[\"MedInc\"]=np.where(df_train[\"MedInc\"]>quantile2,quantile2,df_train[\"MedInc\"])\n",
        "b = round(df_train['MedInc'].skew(),6)\n",
        "print(b)"
      ],
      "metadata": {
        "id": "ZftIxwTL-N8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Interquantile Range\n",
        "quantile1=df_train[\"AveOccup\"].quantile(0.25)\n",
        "quantile2=df_train[\"AveOccup\"].quantile(0.75)\n",
        "df_train[\"AveOccup\"]=np.where(df_train[\"AveOccup\"]<quantile1,quantile1,df_train[\"AveOccup\"])\n",
        "df_train[\"AveOccup\"]=np.where(df_train[\"AveOccup\"]>quantile2,quantile2,df_train[\"AveOccup\"])\n",
        "c = round(df_train['AveOccup'].skew(),6)\n",
        "print(c)"
      ],
      "metadata": {
        "id": "nPGCrD8c-ZB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Interquantile Range\n",
        "quantile1=df_train[\"AveRooms\"].quantile(0.25)\n",
        "quantile2=df_train[\"AveRooms\"].quantile(0.75)\n",
        "df_train[\"AveRooms\"]=np.where(df_train[\"AveRooms\"]<quantile1,quantile1,df_train[\"AveRooms\"])\n",
        "df_train[\"AveRooms\"]=np.where(df_train[\"AveRooms\"]>quantile2,quantile2,df_train[\"AveRooms\"])\n",
        "d = round(df_train['AveRooms'].skew(),6)\n",
        "print(d)"
      ],
      "metadata": {
        "id": "QAGS0K2d-dXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Interquantile Range\n",
        "quantile1=df_train[\"Population\"].quantile(0.25)\n",
        "quantile2=df_train[\"Population\"].quantile(0.75)\n",
        "df_train[\"Population\"]=np.where(df_train[\"Population\"]<quantile1,quantile1,df_train[\"Population\"])\n",
        "df_train[\"Population\"]=np.where(df_train[\"Population\"]>quantile2,quantile2,df_train[\"Population\"])\n",
        "e = round(df_train['Population'].skew(),6)\n",
        "print(e)"
      ],
      "metadata": {
        "id": "GYqZNXNu-lL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Interquantile Range\n",
        "quantile1=df_train[\"MedHouseVal\"].quantile(0.25)\n",
        "quantile2=df_train[\"MedHouseVal\"].quantile(0.75)\n",
        "df_train[\"MedHouseVal\"]=np.where(df_train[\"MedHouseVal\"]<quantile1,quantile1,df_train[\"MedHouseVal\"])\n",
        "df_train[\"MedHouseVal\"]=np.where(df_train[\"MedHouseVal\"]>quantile2,quantile2,df_train[\"MedHouseVal\"])\n",
        "f = round(df_train['MedHouseVal'].skew(),6)\n",
        "print(f)"
      ],
      "metadata": {
        "id": "JDXThdKA-l8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.skew()"
      ],
      "metadata": {
        "id": "RiUh8KqM-t2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformation of target"
      ],
      "metadata": {
        "id": "c6aRObeY-ztN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig,ax = plt.subplots(figsize=(3,3))\n",
        "sns.histplot(np.log(df_train['MedHouseVal']))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4mpbKX-V-xtd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code creates a histogram of the natural logarithm of the \"MedHouseVal\" column in the DataFrame df_train using seaborn.histplot and displays it using matplotlib.pyplot.show. The figsize argument sets the size of the figure to 3x3 inches.\n",
        "\n",
        "Taking the logarithm of a variable is a common transformation that can help to reduce the impact of extreme values and improve the distribution's normality. In this case, taking the logarithm of \"MedHouseVal\" may help to make the distribution more symmetrical and closer to a normal distribution. The resulting histogram can be useful for identifying patterns or trends in the data that may not be as apparent in the original scale."
      ],
      "metadata": {
        "id": "upx5P2Oc_8va"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Correlation** (Train Data)"
      ],
      "metadata": {
        "id": "-qiOsLoqALJB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "correlation = df_train.corr()\n",
        "correlation"
      ],
      "metadata": {
        "id": "nvqVK6kZAC_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code calculates the correlation matrix of the DataFrame df_train using the corr() function from Pandas and assigns the result to the variable correlation.\n",
        "By printing the correlation variable, we can see the correlation coefficients between each pair of variables in the dataset. This information can be useful for identifying relationships between variables and for selecting variables for modeling."
      ],
      "metadata": {
        "id": "J2VQ9M8JC1Ou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "sns.heatmap(correlation,annot=True,cmap='crest',linewidths=0.2)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vBM7A966AX4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code creates a heatmap of the correlation matrix calculated in the previous step using the seaborn.heatmap function and displays it using matplotlib.pyplot.show(). The figsize argument sets the size of the figure to 10x10 inches, while the annot=True argument displays the correlation coefficients in each cell of the heatmap. The cmap argument sets the color scheme to 'crest', while the linewidths argument sets the width of the lines separating each cell to 0.2.\n",
        "The diagonal line of the heatmap shows the correlation of each variable with itself, which is always 1. The heatmap can be a useful tool for quickly identifying strong positive or negative correlations between variables, which can help in feature selection for predictive modeling."
      ],
      "metadata": {
        "id": "seSN00C7C94n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Check for outliers**(Test data)"
      ],
      "metadata": {
        "id": "jYqkW3jtDa3F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.skew()"
      ],
      "metadata": {
        "id": "GQ6Vc-MqAbpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Interquantile Range\n",
        "quantile1=df_test[\"AveBedrms\"].quantile(0.25)\n",
        "quantile2=df_test[\"AveBedrms\"].quantile(0.75)\n",
        "df_test[\"AveBedrms\"]=np.where(df_test[\"AveBedrms\"]<quantile1,quantile1,df_test[\"AveBedrms\"])\n",
        "df_test[\"AveBedrms\"]=np.where(df_test[\"AveBedrms\"]>quantile2,quantile2,df_test[\"AveBedrms\"])\n",
        "a = round(df_test['AveBedrms'].skew(),6)\n",
        "print(a)"
      ],
      "metadata": {
        "id": "kP7j6ByfDlru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Interquantile Range\n",
        "quantile1=df_test[\"AveOccup\"].quantile(0.25)\n",
        "quantile2=df_test[\"AveOccup\"].quantile(0.75)\n",
        "df_test[\"AveOccup\"]=np.where(df_test[\"AveOccup\"]<quantile1,quantile1,df_test[\"AveOccup\"])\n",
        "df_test[\"AveOccup\"]=np.where(df_test[\"AveOccup\"]>quantile2,quantile2,df_test[\"AveOccup\"])\n",
        "b = round(df_test['AveOccup'].skew(),6)\n",
        "print(b)"
      ],
      "metadata": {
        "id": "ECytpD79DvQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Interquantile Range\n",
        "quantile1=df_test[\"AveRooms\"].quantile(0.25)\n",
        "quantile2=df_test[\"AveRooms\"].quantile(0.75)\n",
        "df_test[\"AveRooms\"]=np.where(df_test[\"AveRooms\"]<quantile1,quantile1,df_test[\"AveRooms\"])\n",
        "df_test[\"AveRooms\"]=np.where(df_test[\"AveRooms\"]>quantile2,quantile2,df_test[\"AveRooms\"])\n",
        "c = round(df_test['AveRooms'].skew(),6)\n",
        "print(c)"
      ],
      "metadata": {
        "id": "s53XmOeeDyd8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Interquantile Range\n",
        "quantile1=df_test[\"Population\"].quantile(0.25)\n",
        "quantile2=df_test[\"Population\"].quantile(0.75)\n",
        "df_test[\"Population\"]=np.where(df_test[\"Population\"]<quantile1,quantile1,df_test[\"Population\"])\n",
        "df_test[\"Population\"]=np.where(df_test[\"Population\"]>quantile2,quantile2,df_test[\"Population\"])\n",
        "d = round(df_test['Population'].skew(),6)\n",
        "print(d)"
      ],
      "metadata": {
        "id": "Nfba5geZD1ji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Interquantile Range\n",
        "quantile1=df_test[\"MedInc\"].quantile(0.25)\n",
        "quantile2=df_test[\"MedInc\"].quantile(0.75)\n",
        "df_test[\"MedInc\"]=np.where(df_test[\"MedInc\"]<quantile1,quantile1,df_test[\"MedInc\"])\n",
        "df_test[\"MedInc\"]=np.where(df_test[\"MedInc\"]>quantile2,quantile2,df_test[\"MedInc\"])\n",
        "e = round(df_test['MedInc'].skew(),6)\n",
        "print(e)"
      ],
      "metadata": {
        "id": "RdzGiyZSD5Jr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.skew()"
      ],
      "metadata": {
        "id": "TI8wqGwcD_OB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#correlation(test data)\n",
        "correlation = df_test.corr()\n",
        "correlation"
      ],
      "metadata": {
        "id": "ykWn7lQsEAKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "sns.heatmap(correlation,annot=True,cmap='crest',linewidths=0.2)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "abOaKFmQEI-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Spliting the data**"
      ],
      "metadata": {
        "id": "Mfk4xoK8ERnw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = df_train.iloc[:,1:9]\n",
        "X_train.head(2)"
      ],
      "metadata": {
        "id": "2vIuxLmgEVV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = df_train.MedHouseVal\n",
        "y_train.head(2)"
      ],
      "metadata": {
        "id": "IpRT97muEY6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = df_test.iloc[:,1:]\n",
        "X_test.head(2)"
      ],
      "metadata": {
        "id": "ib8ycTPKEbyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code creates a new DataFrame X_train that contains a subset of columns from the original DataFrame df_train. The iloc function is used to select all rows (:) and columns 1 through 8 (1:9) of df_train. This assumes that the first column of df_train contains the target variable and that the remaining columns contain the predictor variables. The resulting X_train DataFrame contains all rows of df_train but only the predictor variables. \n",
        "It also creates a new y_train that contains the target variable (MedHouseVal) from the original DataFrame df_train. The target variable is the variable we are trying to predict in a predictive modeling task, so we need to separate it from the predictor variables in order to train our model. The y_train Series contains the same number of rows as X_train, with each row corresponding to the target variable value for the corresponding row in X_train.\n"
      ],
      "metadata": {
        "id": "bYnJkJ7gEuOC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Modelling**\n",
        "#####A voting regressor is an ensemble meta-estimator that fits several base regressors, each on the whole dataset. Then it averages the individual predictions to form a final prediction."
      ],
      "metadata": {
        "id": "x9KoY4upFxCp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "reg1 = GradientBoostingRegressor(random_state=1)\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "reg2 = RandomForestRegressor(random_state=1)\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "reg3 = LinearRegression()\n",
        "\n",
        "from sklearn.ensemble import VotingRegressor\n",
        "regressor = VotingRegressor(estimators=[('gb', reg1), ('rf', reg2), ('lir', reg3)])\n",
        "\n",
        "regressor.fit(X_train, y_train)\n",
        "prediction = regressor.predict(X_test)\n",
        "prediction"
      ],
      "metadata": {
        "id": "44KClfs5Fs2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code defines three regression models using scikit-learn: GradientBoostingRegressor, RandomForestRegressor, and LinearRegression. Each model is instantiated with a random_state parameter set to 1, which ensures that the results are reproducible.\n",
        "\n",
        "The code then creates an ensemble of these three models using the VotingRegressor class from scikit-learn. The estimators argument is a list of tuples, where each tuple contains a name for the estimator and the estimator object itself. This ensemble model will make predictions by taking a weighted average of the predictions from each of the three base models.\n",
        "\n",
        "The VotingRegressor object is then fit to the training data (X_train and y_train) using the fit() method. Once the model is trained, it is used to make predictions on the test set (X_test) using the predict() method.\n",
        "\n",
        "The prediction variable contains the predicted target variable values for the test set."
      ],
      "metadata": {
        "id": "wDu9c5QGGPoy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "submission = pd.DataFrame({'id': df_test.id, 'MedHouseVal': prediction})\n",
        "submission.head()"
      ],
      "metadata": {
        "id": "va1kar7WGFbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code creates a new DataFrame submission containing the predicted MedHouseVal values for the test set, along with the id column from the original test set (df_test.id). The pd.DataFrame() function is used to create the DataFrame, with a dictionary of column names and values as the input.\n",
        "\n",
        "The resulting DataFrame has two columns: id and MedHouseVal. The id column contains the same values as the original test set, while the MedHouseVal column contains the predicted values from the ensemble model.\n",
        "\n",
        "Printing the head of submission using submission.head() displays the first five rows of the new DataFrame, allowing us to inspect the predicted values."
      ],
      "metadata": {
        "id": "-tgYkS6dGt-T"
      }
    }
  ]
}